{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e03620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from ckiptagger import construct_dictionary, WS, POS, NER\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "228b4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匯入label好的資料集\n",
    "df = pd.read_csv('labeled_data/2022-01-22.csv', encoding='utf-8', header = None)\n",
    "df.columns=['label', 'text']\n",
    "df.drop(df.index[(df.label != 0) & (df.label != 1) & (df.label != 2)], inplace = True)\n",
    "df.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1ca2f61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>cut_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>先知 尊敬你 酸民哭哭</td>\n",
       "      <td>先知   尊敬 你   酸民 哭哭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>台星現在還是虧錢吧 台哥願意併 有談好條件價碼觀望</td>\n",
       "      <td>台星 現在 還是 虧錢 吧   台哥 願意 併   有 談好 條件 價碼 觀望</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>明天有開盤嗎？總加速師，讓各位韭菜回家吃自己黨宣布經濟成長騰飛8.1%可是同時政府破產又公務...</td>\n",
       "      <td>明天 有 開盤 嗎 ？ 總加速師 ， 讓 各位 韭菜 回家吃自己 黨 宣布 經濟 成長 騰飛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>賺錢特地回來感謝鬆哥 這是很準我是打算買個幾張放著看戲</td>\n",
       "      <td>賺錢 特地 回來 感謝 鬆哥   這 是 很準 我 是 打算 買個幾張放著 看戲</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>其實這題材還行 畢竟他算是 電動車跟元宇宙都有涉獵到 但什麼時候可以廣泛應用  還說不準 怕...</td>\n",
       "      <td>其實 這 題材 還 行   畢竟 他 算是   電動車 跟 元 宇宙 都 有 涉獵到   但...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4719</th>\n",
       "      <td>0.0</td>\n",
       "      <td>※ 引述《thumbd000 (藤原千花)》之銘言： : 從國際股市來看 : 美股爆殺到現在...</td>\n",
       "      <td>※  引述 《 thumbd 000   ( 藤原千花 ) 》 之 銘言 ：  :   從 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4720</th>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>2.0</td>\n",
       "      <td>不錯阿，說得很好</td>\n",
       "      <td>不錯 阿 ， 說 得 很好</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4722</th>\n",
       "      <td>2.0</td>\n",
       "      <td>大陸之前打通膨是為了之後的寬鬆政策吧?熱錢都流到大陸也不用怕通膨美國升息縮表都不能把錢從大陸吸走?</td>\n",
       "      <td>大陸 之前 打通膨 是 為 了 之後 的 寬鬆 政策 吧 ? 熱錢 都 流到 大陸 也 不用...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4723</th>\n",
       "      <td>2.0</td>\n",
       "      <td>中國超前部署去槓桿 今年獨漲外資要報復性買盤中概股了</td>\n",
       "      <td>中國 超前 部署 去 槓桿   今年 獨漲 外資 要 報復性 買盤 中 概股 了</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4724 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0       2.0                                        先知 尊敬你 酸民哭哭   \n",
       "1       0.0                          台星現在還是虧錢吧 台哥願意併 有談好條件價碼觀望   \n",
       "2       0.0  明天有開盤嗎？總加速師，讓各位韭菜回家吃自己黨宣布經濟成長騰飛8.1%可是同時政府破產又公務...   \n",
       "3       1.0                        賺錢特地回來感謝鬆哥 這是很準我是打算買個幾張放著看戲   \n",
       "4       1.0  其實這題材還行 畢竟他算是 電動車跟元宇宙都有涉獵到 但什麼時候可以廣泛應用  還說不準 怕...   \n",
       "...     ...                                                ...   \n",
       "4719    0.0  ※ 引述《thumbd000 (藤原千花)》之銘言： : 從國際股市來看 : 美股爆殺到現在...   \n",
       "4720    2.0                                                ...   \n",
       "4721    2.0                                           不錯阿，說得很好   \n",
       "4722    2.0  大陸之前打通膨是為了之後的寬鬆政策吧?熱錢都流到大陸也不用怕通膨美國升息縮表都不能把錢從大陸吸走?   \n",
       "4723    2.0                         中國超前部署去槓桿 今年獨漲外資要報復性買盤中概股了   \n",
       "\n",
       "                                               cut_text  \n",
       "0                                     先知   尊敬 你   酸民 哭哭  \n",
       "1               台星 現在 還是 虧錢 吧   台哥 願意 併   有 談好 條件 價碼 觀望  \n",
       "2     明天 有 開盤 嗎 ？ 總加速師 ， 讓 各位 韭菜 回家吃自己 黨 宣布 經濟 成長 騰飛...  \n",
       "3              賺錢 特地 回來 感謝 鬆哥   這 是 很準 我 是 打算 買個幾張放著 看戲  \n",
       "4     其實 這 題材 還 行   畢竟 他 算是   電動車 跟 元 宇宙 都 有 涉獵到   但...  \n",
       "...                                                 ...  \n",
       "4719  ※  引述 《 thumbd 000   ( 藤原千花 ) 》 之 銘言 ：  :   從 ...  \n",
       "4720                                                ...  \n",
       "4721                                      不錯 阿 ， 說 得 很好  \n",
       "4722  大陸 之前 打通膨 是 為 了 之後 的 寬鬆 政策 吧 ? 熱錢 都 流到 大陸 也 不用...  \n",
       "4723           中國 超前 部署 去 槓桿   今年 獨漲 外資 要 報復性 買盤 中 概股 了  \n",
       "\n",
       "[4724 rows x 3 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9bdf4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匯入stopwords\n",
    "def import_stopwords():\n",
    "    # 匯入stopwords\n",
    "    with open(\"stopwords.csv\",'r',encoding='utf-8') as f:\n",
    "        stopword_file = pd.read_csv(f, header=None)\n",
    "    stopwords_list = stopword_file.values.tolist()\n",
    "    stopwords = set()\n",
    "    for s in stopwords_list:\n",
    "        for word in s:\n",
    "            if pd.isnull(word) == False:\n",
    "                if type(word) == float:\n",
    "                    word = str(int(word))\n",
    "                stopwords.add(word)\n",
    "    return list(stopwords)\n",
    "stopwords = import_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8b9382f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    }
   ],
   "source": [
    "# 使用ckip斷詞\n",
    "def ckip(sentence):\n",
    "    # 匯入自定斷詞\n",
    "    with open('customized.csv','r',encoding='utf-8') as f:\n",
    "        customized_file = pd.read_csv(f, header=None)\n",
    "    customized_list = customized_file.values.tolist()\n",
    "    \n",
    "    # 整理customized格式\n",
    "    customized = set()\n",
    "    for c in customized_list:\n",
    "        for word in c:\n",
    "            if pd.isnull(word) == False:\n",
    "                if type(word) == float:\n",
    "                    word = str(int(word))\n",
    "                customized.add(word)\n",
    "    customized.update(stopwords)\n",
    "    customized = list(customized)\n",
    "\n",
    "    # 給斷詞權重   \n",
    "    word_to_weight = {}\n",
    "    for c in customized:\n",
    "        word_to_weight.update({c : 1})\n",
    "    \n",
    "    # 利用ckip斷詞\n",
    "    dictionary = construct_dictionary( word_to_weight )\n",
    "    ws = WS(\"./data\") \n",
    "    ws_results = ws(sentence, \n",
    "                    # recommend_dictionary = dictionary,\n",
    "                    coerce_dictionary = dictionary)\n",
    "    return ws_results\n",
    "ws_results = ckip(df.text)\n",
    "\n",
    "# 將斷詞完成的資料存成dataframe\n",
    "cut_text = [\" \".join(i) for i in ws_results]\n",
    "df['cut_text'] = pd.DataFrame(cut_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a0bdae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分別存入 X 跟 y\n",
    "X = df[['cut_text']]\n",
    "y = df.label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2e5f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "de0aff34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3543, 7391)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(X_train.cut_text).toarray(), \n",
    "                           columns = vect.get_feature_names_out())\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3af58754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['c螺', 'ettoday', '其他', '助詞', '動詞', '名詞', '單位量詞冠詞', '大家好我是will', '感嘆情緒', '成語俚語', '時間', '柯p', '稱呼人名', '符號', '處份', '財經m平方', '連接詞'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3543, 7136)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words = frozenset(stopwords))\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(X_train.cut_text).toarray(), \n",
    "                           columns = vect.get_feature_names_out())\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "926a06a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['c螺', 'ettoday', '其他', '助詞', '動詞', '名詞', '單位量詞冠詞', '大家好我是will', '感嘆情緒', '成語俚語', '時間', '柯p', '稱呼人名', '符號', '處份', '財經m平方', '連接詞'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0050</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>...</th>\n",
       "      <th>鰻魚</th>\n",
       "      <th>鴻海</th>\n",
       "      <th>黃金</th>\n",
       "      <th>默默</th>\n",
       "      <th>點位</th>\n",
       "      <th>點名</th>\n",
       "      <th>點擊</th>\n",
       "      <th>鼓勵</th>\n",
       "      <th>龍頭</th>\n",
       "      <th>龍頭股</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 1764 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     00  000  0050  01  02  03  04  05  06  07  ...  鰻魚  鴻海  黃金  默默  點位  點名  \\\n",
       "0     0    0     0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
       "1     0    0     0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
       "2     0    0     0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
       "3     0    0     0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
       "4     0    0     0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
       "..   ..  ...   ...  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..   \n",
       "395   0    0     0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
       "396   0    0     0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
       "397   0    0     0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
       "398   0    0     0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
       "399   0    0     0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
       "\n",
       "     點擊  鼓勵  龍頭  龍頭股  \n",
       "0     0   0   0    0  \n",
       "1     0   0   0    0  \n",
       "2     0   0   0    0  \n",
       "3     0   0   0    0  \n",
       "4     0   0   0    0  \n",
       "..   ..  ..  ..  ...  \n",
       "395   0   0   0    0  \n",
       "396   0   0   0    0  \n",
       "397   0   0   0    0  \n",
       "398   0   0   0    0  \n",
       "399   0   0   0    0  \n",
       "\n",
       "[400 rows x 1764 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df = 0.8,\n",
    "                       min_df = 3,\n",
    "                       stop_words = frozenset(stopwords))\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(X_train.cut_text).toarray(), \n",
    "                           columns = vect.get_feature_names_out())\n",
    "term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "da957da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['c螺', 'ettoday', '其他', '助詞', '動詞', '名詞', '單位量詞冠詞', '大家好我是will', '感嘆情緒', '成語俚語', '時間', '柯p', '稱呼人名', '符號', '處份', '財經m平方', '連接詞'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['c螺', 'ettoday', '其他', '助詞', '動詞', '名詞', '單位量詞冠詞', '大家好我是will', '感嘆情緒', '成語俚語', '時間', '柯p', '稱呼人名', '符號', '處份', '財經m平方', '連接詞'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['c螺', 'ettoday', '其他', '助詞', '動詞', '名詞', '單位量詞冠詞', '大家好我是will', '感嘆情緒', '成語俚語', '時間', '柯p', '稱呼人名', '符號', '處份', '財經m平方', '連接詞'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['c螺', 'ettoday', '其他', '助詞', '動詞', '名詞', '單位量詞冠詞', '大家好我是will', '感嘆情緒', '成語俚語', '時間', '柯p', '稱呼人名', '符號', '處份', '財經m平方', '連接詞'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6429557823942371\n",
      "0.6511430990685859\n",
      "[[102   6 200]\n",
      " [ 27  11 100]\n",
      " [ 59  20 656]]\n",
      "0.24850003197901432\n",
      "0.6048197337073877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['c螺', 'ettoday', '其他', '助詞', '動詞', '名詞', '單位量詞冠詞', '大家好我是will', '感嘆情緒', '成語俚語', '時間', '柯p', '稱呼人名', '符號', '處份', '財經m平方', '連接詞'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "pipe = make_pipeline(vect, nb)\n",
    "print(cross_val_score(pipe, X_train.cut_text, y_train, cv = 5, scoring='accuracy').mean())\n",
    "pipe.fit(X_train.cut_text, y_train)\n",
    "pipe.predict(X_test.cut_text)\n",
    "y_pred = pipe.predict(X_test.cut_text)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.matthews_corrcoef(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "31ebd3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 建立 XGBClassifier 模型\n",
    "xgboostModel = XGBClassifier(n_estimators=100, learning_rate= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bb77a8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;,\n",
       "                 CountVectorizer(max_df=0.8, min_df=3,\n",
       "                                 stop_words=frozenset({&#x27; -&#x27;, &#x27; :// &#x27;, &#x27; =&#x27;,\n",
       "                                                       &#x27; ettoday&#x27;, &#x27;!&#x27;, &#x27;#&#x27;,\n",
       "                                                       &#x27;$&#x27;, &#x27;%&#x27;, &#x27;&amp;&#x27;, &#x27;(&#x27;, &#x27;)&#x27;,\n",
       "                                                       &#x27;*&#x27;, &#x27;+&#x27;, &#x27;-&#x27;, &#x27;--&#x27;,\n",
       "                                                       &#x27;.\\n&#x27;, &#x27;..&#x27;, &#x27;...&#x27;,\n",
       "                                                       &#x27;....&#x27;, &#x27;......&#x27;, &#x27;./&#x27;,\n",
       "                                                       &#x27;/&#x27;, &#x27;//&#x27;, &#x27;///&#x27;, &#x27;57台&#x27;,\n",
       "                                                       &#x27;6666哥&#x27;, &#x27;:&#x27;, &#x27;:: &#x27;, &#x27;;&#x27;,\n",
       "                                                       &#x27;&lt;&#x27;, ...}))),\n",
       "                (&#x27;xgbclassifier&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                               colsample_bylevel=...\n",
       "                               gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "                               importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "                               learning_rate=0.3, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=&#x27;()&#x27;,\n",
       "                               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "                               objective=&#x27;multi:softprob&#x27;, predictor=&#x27;auto&#x27;,\n",
       "                               random_state=0, reg_alpha=0, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;,\n",
       "                 CountVectorizer(max_df=0.8, min_df=3,\n",
       "                                 stop_words=frozenset({&#x27; -&#x27;, &#x27; :// &#x27;, &#x27; =&#x27;,\n",
       "                                                       &#x27; ettoday&#x27;, &#x27;!&#x27;, &#x27;#&#x27;,\n",
       "                                                       &#x27;$&#x27;, &#x27;%&#x27;, &#x27;&amp;&#x27;, &#x27;(&#x27;, &#x27;)&#x27;,\n",
       "                                                       &#x27;*&#x27;, &#x27;+&#x27;, &#x27;-&#x27;, &#x27;--&#x27;,\n",
       "                                                       &#x27;.\\n&#x27;, &#x27;..&#x27;, &#x27;...&#x27;,\n",
       "                                                       &#x27;....&#x27;, &#x27;......&#x27;, &#x27;./&#x27;,\n",
       "                                                       &#x27;/&#x27;, &#x27;//&#x27;, &#x27;///&#x27;, &#x27;57台&#x27;,\n",
       "                                                       &#x27;6666哥&#x27;, &#x27;:&#x27;, &#x27;:: &#x27;, &#x27;;&#x27;,\n",
       "                                                       &#x27;&lt;&#x27;, ...}))),\n",
       "                (&#x27;xgbclassifier&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                               colsample_bylevel=...\n",
       "                               gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "                               importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "                               learning_rate=0.3, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=&#x27;()&#x27;,\n",
       "                               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "                               objective=&#x27;multi:softprob&#x27;, predictor=&#x27;auto&#x27;,\n",
       "                               random_state=0, reg_alpha=0, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_df=0.8, min_df=3,\n",
       "                stop_words=frozenset({&#x27; -&#x27;, &#x27; :// &#x27;, &#x27; =&#x27;, &#x27; ettoday&#x27;, &#x27;!&#x27;, &#x27;#&#x27;,\n",
       "                                      &#x27;$&#x27;, &#x27;%&#x27;, &#x27;&amp;&#x27;, &#x27;(&#x27;, &#x27;)&#x27;, &#x27;*&#x27;, &#x27;+&#x27;, &#x27;-&#x27;,\n",
       "                                      &#x27;--&#x27;, &#x27;.\\n&#x27;, &#x27;..&#x27;, &#x27;...&#x27;, &#x27;....&#x27;,\n",
       "                                      &#x27;......&#x27;, &#x27;./&#x27;, &#x27;/&#x27;, &#x27;//&#x27;, &#x27;///&#x27;, &#x27;57台&#x27;,\n",
       "                                      &#x27;6666哥&#x27;, &#x27;:&#x27;, &#x27;:: &#x27;, &#x27;;&#x27;, &#x27;&lt;&#x27;, ...}))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=&#x27;auto&#x27;, random_state=0, reg_alpha=0, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('countvectorizer',\n",
       "                 CountVectorizer(max_df=0.8, min_df=3,\n",
       "                                 stop_words=frozenset({' -', ' :// ', ' =',\n",
       "                                                       ' ettoday', '!', '#',\n",
       "                                                       '$', '%', '&', '(', ')',\n",
       "                                                       '*', '+', '-', '--',\n",
       "                                                       '.\\n', '..', '...',\n",
       "                                                       '....', '......', './',\n",
       "                                                       '/', '//', '///', '57台',\n",
       "                                                       '6666哥', ':', ':: ', ';',\n",
       "                                                       '<', ...}))),\n",
       "                ('xgbclassifier',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                               colsample_bylevel=...\n",
       "                               gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                               importance_type=None, interaction_constraints='',\n",
       "                               learning_rate=0.3, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints='()',\n",
       "                               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', predictor='auto',\n",
       "                               random_state=0, reg_alpha=0, ...))])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用訓練資料訓練模型\n",
    "xgb = xgboostModel\n",
    "pipe = make_pipeline(vect, xgb)\n",
    "pipe.fit(X_train.cut_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "34177d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集:  0.7567033587355348\n",
      "測試集:  0.6613039796782387\n"
     ]
    }
   ],
   "source": [
    "# 使用訓練資料預測分類\n",
    "predicted = pipe.predict(X_train.cut_text)\n",
    "print('訓練集: ',pipe.score(X_train.cut_text,y_train))\n",
    "print('測試集: ',pipe.score(X_test.cut_text,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
